{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"12\">COMPSCI 762 Assignment 01</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">Name: Sihui Yang</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from matplotlib import pyplot\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_dataset = pd.read_csv('arrhythmia.csv', na_values='?')\n",
    "bcp_dataset = pd.read_csv('BCP.csv', na_values='?')\n",
    "web_dataset = pd.read_csv('website-phishing.csv', na_values='?')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">Task 1:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrhythmia NA values:  408\n",
      "BCP NA values:  0\n",
      "web-phishing NA values:  0\n"
     ]
    }
   ],
   "source": [
    "#arrhythmia\n",
    "arr_missing_value = arr_dataset.isna().sum()\n",
    "arr_missing_value_count = arr_missing_value.sum()\n",
    "print(\"arrhythmia NA values: \", arr_missing_value_count)\n",
    "#BCP\n",
    "bcp_missing_value = bcp_dataset.isna().sum()\n",
    "bcp_missing_value_count = bcp_missing_value.sum()\n",
    "print(\"BCP NA values: \", bcp_missing_value_count)\n",
    "#web\n",
    "web_missing_value = web_dataset.isna().sum()\n",
    "web_missing_value_count = web_missing_value.sum()\n",
    "print(\"web-phishing NA values: \", web_missing_value_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " T : 8\n",
      " P : 22\n",
      " QRST : 1\n",
      " J : 376\n",
      " heartrate : 1\n"
     ]
    }
   ],
   "source": [
    "for row, col in arr_dataset.items():\n",
    "    col_na = col.isna().sum()\n",
    "    if col_na != 0:\n",
    "        print(row, \":\", col_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillNA = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "arr_dataset = pd.DataFrame(fillNA.fit_transform(arr_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'>First we read all the dataset and then start to deal with missing values, as mentioned in the task, replace the value with the mean. We first convert the \"?\" in the dataset to NaN so that we can check the number of missing values in each dataset. We found missing values only in \"arrhythmia.csv\", so we only need to deal with this one dataset. Then we find which attributes contain missing values in the dataset, and use SimpleImputer() and fit_transform() to replace the missing values in that attribute with the average of these attributes. Using fit() and transform() respectively will reduce the efficiency of the model.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">Task 2:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_dataset_X_train, arr_dataset_X_test, arr_dataset_y_train, arr_dataset_y_test = train_test_split(arr_dataset.iloc[:,:-1], arr_dataset.iloc[:,-1], test_size=0.2, random_state = 1221)\n",
    "bcp_dataset_X_train, bcp_dataset_X_test, bcp_dataset_y_train, bcp_dataset_y_test = train_test_split(bcp_dataset.iloc[:,:-1], bcp_dataset.iloc[:,-1], test_size=0.2, random_state = 1221)\n",
    "web_dataset_X_train, web_dataset_X_test, web_dataset_y_train, web_dataset_y_test = train_test_split(web_dataset.iloc[:,:-1], web_dataset.iloc[:,-1], test_size=0.2, random_state = 1221)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desicion Stump\n",
      "----*****-----\n",
      "arrhythmia dataset test set accuracy:  0.4725274725274725\n",
      "BCP dataset test set accuracy:  0.927007299270073\n",
      "web-phishing dataset test set accuracy:  0.8959746720940751\n"
     ]
    }
   ],
   "source": [
    "#Decision stump\n",
    "print(\"Desicion Stump\")\n",
    "print(\"----*****-----\")\n",
    "#arrhythmia\n",
    "arr_clf = tree.DecisionTreeClassifier(max_depth=1 ,criterion=\"entropy\", random_state = 1221)\n",
    "arr_clf = arr_clf.fit(arr_dataset_X_train, arr_dataset_y_train)\n",
    "arr_y_pred = arr_clf.predict(arr_dataset_X_test)\n",
    "arr_test_acc_ds = metrics.accuracy_score(arr_dataset_y_test, arr_y_pred)\n",
    "print(\"arrhythmia dataset test set accuracy: \", arr_test_acc_ds)\n",
    "#BCP\n",
    "bcp_clf = tree.DecisionTreeClassifier(max_depth=1, criterion=\"entropy\", random_state = 1221)\n",
    "bcp_clf = bcp_clf.fit(bcp_dataset_X_train, bcp_dataset_y_train)\n",
    "bcp_y_pred = bcp_clf.predict(bcp_dataset_X_test)\n",
    "bcp_test_acc_ds = metrics.accuracy_score(bcp_dataset_y_test, bcp_y_pred)\n",
    "print(\"BCP dataset test set accuracy: \", bcp_test_acc_ds)\n",
    "#web\n",
    "web_clf = tree.DecisionTreeClassifier(max_depth=1, criterion=\"entropy\", random_state = 1221)\n",
    "web_clf = web_clf.fit(web_dataset_X_train, web_dataset_y_train)\n",
    "web_y_pred = web_clf.predict(web_dataset_X_test)\n",
    "web_test_acc_ds = metrics.accuracy_score(web_dataset_y_test, web_y_pred)\n",
    "print(\"web-phishing dataset test set accuracy: \", web_test_acc_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpruned Desicion Stump\n",
      "----*****-----\n",
      "arrhythmia dataset test set accuracy:  0.6373626373626373\n",
      "BCP dataset test set accuracy:  0.9416058394160584\n",
      "web-phishing dataset test set accuracy:  0.9624604251469923\n"
     ]
    }
   ],
   "source": [
    "#Unpruned decision tree\n",
    "print(\"Unpruned Desicion Stump\")\n",
    "print(\"----*****-----\")\n",
    "#arrhythmia\n",
    "arr_clf = tree.DecisionTreeClassifier()\n",
    "arr_clf = arr_clf.fit(arr_dataset_X_train, arr_dataset_y_train)\n",
    "arr_y_pred = arr_clf.predict(arr_dataset_X_test)\n",
    "arr_test_acc_uds = metrics.accuracy_score(arr_dataset_y_test, arr_y_pred)\n",
    "print(\"arrhythmia dataset test set accuracy: \", arr_test_acc_uds)\n",
    "#BCP\n",
    "bcp_clf = tree.DecisionTreeClassifier()\n",
    "bcp_clf = bcp_clf.fit(bcp_dataset_X_train, bcp_dataset_y_train)\n",
    "bcp_y_pred = bcp_clf.predict(bcp_dataset_X_test)\n",
    "bcp_test_acc_uds = metrics.accuracy_score(bcp_dataset_y_test, bcp_y_pred)\n",
    "print(\"BCP dataset test set accuracy: \", bcp_test_acc_uds)\n",
    "#web\n",
    "web_clf = tree.DecisionTreeClassifier()\n",
    "web_clf = web_clf.fit(web_dataset_X_train, web_dataset_y_train)\n",
    "web_y_pred = web_clf.predict(web_dataset_X_test)\n",
    "web_test_acc_uds = metrics.accuracy_score(web_dataset_y_test, web_y_pred)\n",
    "print(\"web-phishing dataset test set accuracy: \", web_test_acc_uds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Desicion Stump\n",
      "----*****-----\n",
      "arrhythmia dataset test set accuracy:  0.6593406593406593\n",
      "BCP dataset test set accuracy:  0.9635036496350365\n",
      "web-phishing dataset test set accuracy:  0.9176843057440073\n"
     ]
    }
   ],
   "source": [
    "#Pruned decision tree\n",
    "print(\"Pruned Desicion Stump\")\n",
    "print(\"----*****-----\")\n",
    "#arrhythmia\n",
    "arr_clf = tree.DecisionTreeClassifier(max_depth=5 ,criterion=\"entropy\", random_state = 1221)\n",
    "arr_clf = arr_clf.fit(arr_dataset_X_train, arr_dataset_y_train)\n",
    "arr_y_pred = arr_clf.predict(arr_dataset_X_test)\n",
    "arr_test_acc_pds = metrics.accuracy_score(arr_dataset_y_test, arr_y_pred)\n",
    "print(\"arrhythmia dataset test set accuracy: \", arr_test_acc_pds)\n",
    "#BCP\n",
    "bcp_clf = tree.DecisionTreeClassifier(max_depth=5 ,criterion=\"entropy\", random_state = 1221)\n",
    "bcp_clf = bcp_clf.fit(bcp_dataset_X_train, bcp_dataset_y_train)\n",
    "bcp_y_pred = bcp_clf.predict(bcp_dataset_X_test)\n",
    "bcp_test_acc_pds = metrics.accuracy_score(bcp_dataset_y_test, bcp_y_pred)\n",
    "print(\"BCP dataset test set accuracy: \", bcp_test_acc_pds)\n",
    "#web\n",
    "web_clf = tree.DecisionTreeClassifier(max_depth=5 ,criterion=\"entropy\", random_state = 1221)\n",
    "web_clf = web_clf.fit(web_dataset_X_train, web_dataset_y_train)\n",
    "web_y_pred = web_clf.predict(web_dataset_X_test)\n",
    "web_test_acc_pds = metrics.accuracy_score(web_dataset_y_test, web_y_pred)\n",
    "print(\"web-phishing dataset test set accuracy: \", web_test_acc_pds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'>We split each dataset into training and testing data.\n",
    " For decision stumps, which simply have only 1 rule based on only 1 feature, we just need to set the value of max_depth to 1.\n",
    " For the unpruned decision tree, we let it grow freely, so do not set any value.\n",
    " For the pruned decision tree, this time we will use pre-pruning, which helps reduce overfitting by stopping the tree before it has finished classifying the training set. For this task, we set the hyperparameter value to 5 arbitrarily for now.\n",
    " We use accuracy_score() to calculate the accuracy of the test set of each model.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">Task 3:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "----*****-----\n",
      "arrhythmia dataset best estimator:  5\n",
      "arrhythmia dataset test set accuracy:  0.6593406593406593\n",
      "----*****-----\n",
      "BCP dataset best estimator:  22\n",
      "BCP dataset test set accuracy:  0.9708029197080292\n",
      "----*****-----\n",
      "web-phishing dataset best estimator:  27\n",
      "web-phishing dataset test set accuracy:  0.9656264133876075\n"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "    \"max_depth\": range(2, 30)\n",
    "}\n",
    "print(\"Hyperparameters\")\n",
    "print(\"----*****-----\")\n",
    "max_arr_clf = tree.DecisionTreeClassifier()\n",
    "#arrhythmia\n",
    "arr_gscv = GridSearchCV(estimator = max_arr_clf, param_grid = search_space, cv=KFold(n_splits=10))\n",
    "arr_gscv.fit(arr_dataset_X_train, arr_dataset_y_train)\n",
    "arr_best_hype = arr_gscv.best_estimator_\n",
    "print(\"arrhythmia dataset best estimator: \", arr_best_hype.max_depth)\n",
    "#calculate the accracy on the best estimator\n",
    "arr_clf = tree.DecisionTreeClassifier(max_depth= arr_best_hype.max_depth, criterion=\"entropy\", random_state = 1221)\n",
    "arr_clf = arr_clf.fit(arr_dataset_X_train, arr_dataset_y_train)\n",
    "arr_y_pred = arr_clf.predict(arr_dataset_X_test)\n",
    "arr_test_acc = metrics.accuracy_score(arr_dataset_y_test, arr_y_pred)\n",
    "print(\"arrhythmia dataset test set accuracy: \", arr_test_acc)\n",
    "print(\"----*****-----\")\n",
    "#BCP\n",
    "bcp_gscv = GridSearchCV(estimator = max_arr_clf, param_grid = search_space, cv=KFold(n_splits=10))\n",
    "bcp_gscv.fit(bcp_dataset_X_train, bcp_dataset_y_train)\n",
    "bcp_best_hype = bcp_gscv.best_estimator_\n",
    "print(\"BCP dataset best estimator: \", bcp_best_hype.max_depth)\n",
    "##calculate the accracy on the best estimator\n",
    "bcp_clf = tree.DecisionTreeClassifier(max_depth= bcp_best_hype.max_depth,criterion=\"entropy\", random_state = 1221)\n",
    "bcp_clf = bcp_clf.fit(bcp_dataset_X_train, bcp_dataset_y_train)\n",
    "bcp_y_pred = bcp_clf.predict(bcp_dataset_X_test)\n",
    "bcp_test_acc = metrics.accuracy_score(bcp_dataset_y_test, bcp_y_pred)\n",
    "print(\"BCP dataset test set accuracy: \", bcp_test_acc)\n",
    "print(\"----*****-----\")\n",
    "#web\n",
    "web_gscv = GridSearchCV(estimator = max_arr_clf, param_grid = search_space, cv=KFold(n_splits=10))\n",
    "web_gscv.fit(web_dataset_X_train, web_dataset_y_train)\n",
    "web_best_hype = web_gscv.best_estimator_\n",
    "print(\"web-phishing dataset best estimator: \", web_best_hype.max_depth)\n",
    "#calculate the accracy on the best estimator\n",
    "web_clf = tree.DecisionTreeClassifier(max_depth= web_best_hype.max_depth,criterion=\"entropy\", random_state = 1221)\n",
    "web_clf = web_clf.fit(web_dataset_X_train, web_dataset_y_train)\n",
    "web_y_pred = web_clf.predict(web_dataset_X_test)\n",
    "web_test_acc = metrics.accuracy_score(web_dataset_y_test, web_y_pred)\n",
    "print(\"web-phishing dataset test set accuracy: \", web_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr shape:  (452, 280)\n",
      "bcp shape:  (683, 11)\n",
      "web shape:  (11055, 31)\n"
     ]
    }
   ],
   "source": [
    "print(\"arr shape: \", arr_dataset.shape)\n",
    "print(\"bcp shape: \", bcp_dataset.shape)\n",
    "print(\"web shape: \", web_dataset.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"4\">We need to calculate hyperparameters for the pruned decision tree. We selected hyperparameter values using a grid search, which performs an exhaustive search of the estimator for specified parameter values. To reduce optimization bias, we calculated each hyperparameter value by computing the 10-fold cross-validation accuracy for each model. We also used best_estimator_ to perform an exhaustive search for specified parameter values of the estimator. Finally we calculate the accuracy again based on the computed estimator.\n",
    "\n",
    "We can see that the \"arrhythmia.csv\" dataset has the shallowest tree, but it has the highest number of attributes and the lowest accuracy. This may be a result of model overfitting, as the model has a low max depth as the tree gets deeper and insufficient samples are available to utilize all attributes effectively for classification purposes. \n",
    "\n",
    "The \"BCP.csv\" dataset has the fewest attribute values, and the highest accurate values. This may be due to the small number of attributes and the model can learn more efficiently. \n",
    "\n",
    "The \"web-phishing.csv\" dataset has the largest hyperparameter values. We can find the answer from the data in the \"web-phishing.csv\" dataset, because these 31 attributes are all binary or ternary data, and these attributes can only be split once or twice at most.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">Task 4:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caculate the mean of each model of each dataset\n",
    "#Decision stump\n",
    "amean_ds = arr_test_acc_ds.mean() #arrhythmia\n",
    "bmean_ds = bcp_test_acc_ds.mean() #BCP\n",
    "wmean_ds = web_test_acc_ds.mean() #web\n",
    "#Unpruned decision tree\n",
    "amean_uds = arr_test_acc_uds.mean() #arrhythmia\n",
    "bmean_uds = bcp_test_acc_uds.mean() #BCP\n",
    "wmean_uds = web_test_acc_uds.mean() #web\n",
    "#Pruned decision tree\n",
    "amean_pds = arr_test_acc_pds.mean() #arrhythmia\n",
    "bmean_pds = bcp_test_acc_pds.mean() #BCP\n",
    "wmean_pds = web_test_acc_pds.mean() #web\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_ds = []\n",
    "arr_uds = []\n",
    "arr_pds = []\n",
    "i = 1\n",
    "for i in range(1, 20):\n",
    "    arr_dataset_X_train, arr_dataset_X_test, arr_dataset_y_train, arr_dataset_y_test = train_test_split(arr_dataset.iloc[:,:-1], arr_dataset.iloc[:,-1], test_size=0.2, random_state = i)\n",
    "    arr_clf = tree.DecisionTreeClassifier(max_depth= 1 ,criterion=\"entropy\", random_state = 1221)\n",
    "    arr_clf = arr_clf.fit(arr_dataset_X_train, arr_dataset_y_train)\n",
    "    arr_y_pred = arr_clf.predict(arr_dataset_X_test)\n",
    "    arr_test_acc_ds = metrics.accuracy_score(arr_dataset_y_test, arr_y_pred)\n",
    "    arr_ds.append(arr_test_acc_ds)\n",
    "    i += 1\n",
    "for i in range(1, 20):\n",
    "    arr_dataset_X_train, arr_dataset_X_test, arr_dataset_y_train, arr_dataset_y_test = train_test_split(arr_dataset.iloc[:,:-1], arr_dataset.iloc[:,-1], test_size=0.2, random_state = i)\n",
    "    arr_clf = tree.DecisionTreeClassifier()\n",
    "    arr_clf = arr_clf.fit(arr_dataset_X_train, arr_dataset_y_train)\n",
    "    arr_y_pred = arr_clf.predict(arr_dataset_X_test)\n",
    "    arr_test_acc_uds = metrics.accuracy_score(arr_dataset_y_test, arr_y_pred)\n",
    "    arr_uds.append(arr_test_acc_uds)\n",
    "    i += 1\n",
    "for i in range(1, 20):\n",
    "    arr_dataset_X_train, arr_dataset_X_test, arr_dataset_y_train, arr_dataset_y_test = train_test_split(arr_dataset.iloc[:,:-1], arr_dataset.iloc[:,-1], test_size=0.2, random_state = i)\n",
    "    arr_clf = tree.DecisionTreeClassifier(max_depth= 5 ,criterion=\"entropy\", random_state = 1221)\n",
    "    arr_clf = arr_clf.fit(arr_dataset_X_train, arr_dataset_y_train)\n",
    "    arr_y_pred = arr_clf.predict(arr_dataset_X_test)\n",
    "    arr_test_acc_pds = metrics.accuracy_score(arr_dataset_y_test, arr_y_pred)\n",
    "    arr_pds.append(arr_test_acc_pds)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcp_ds = []\n",
    "bcp_uds = []\n",
    "bcp_pds = []\n",
    "i = 1\n",
    "for i in range(1, 20):\n",
    "    bcp_dataset_X_train, bcp_dataset_X_test, bcp_dataset_y_train, bcp_dataset_y_test = train_test_split(bcp_dataset.iloc[:,:-1], bcp_dataset.iloc[:,-1], test_size=0.2, random_state = i)\n",
    "    bcp_clf = tree.DecisionTreeClassifier(max_depth=1, criterion=\"entropy\", random_state = 1221)\n",
    "    bcp_clf = bcp_clf.fit(bcp_dataset_X_train, bcp_dataset_y_train)\n",
    "    bcp_y_pred = bcp_clf.predict(bcp_dataset_X_test)\n",
    "    bcp_test_acc_ds = metrics.accuracy_score(bcp_dataset_y_test, bcp_y_pred)\n",
    "    bcp_ds.append(bcp_test_acc_ds)\n",
    "    i += 1\n",
    "for i in range(1, 20):\n",
    "    bcp_dataset_X_train, bcp_dataset_X_test, bcp_dataset_y_train, bcp_dataset_y_test = train_test_split(bcp_dataset.iloc[:,:-1], bcp_dataset.iloc[:,-1], test_size=0.2, random_state = i)\n",
    "    bcp_clf = tree.DecisionTreeClassifier()\n",
    "    bcp_clf = bcp_clf.fit(bcp_dataset_X_train, bcp_dataset_y_train)\n",
    "    bcp_y_pred = bcp_clf.predict(bcp_dataset_X_test)\n",
    "    bcp_test_acc_uds = metrics.accuracy_score(bcp_dataset_y_test, bcp_y_pred)\n",
    "    bcp_uds.append(bcp_test_acc_uds)\n",
    "    i += 1\n",
    "for i in range(1, 20):\n",
    "    bcp_dataset_X_train, bcp_dataset_X_test, bcp_dataset_y_train, bcp_dataset_y_test = train_test_split(bcp_dataset.iloc[:,:-1], bcp_dataset.iloc[:,-1], test_size=0.2, random_state = i)\n",
    "    bcp_clf = tree.DecisionTreeClassifier(max_depth=5 ,criterion=\"entropy\", random_state = 1221)\n",
    "    bcp_clf = bcp_clf.fit(bcp_dataset_X_train, bcp_dataset_y_train)\n",
    "    bcp_y_pred = bcp_clf.predict(bcp_dataset_X_test)\n",
    "    bcp_test_acc_pds = metrics.accuracy_score(bcp_dataset_y_test, bcp_y_pred)\n",
    "    bcp_pds.append(bcp_test_acc_pds)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_ds = []\n",
    "web_uds = []\n",
    "web_pds = []\n",
    "i = 1\n",
    "for i in range(1, 20):\n",
    "    web_dataset_X_train, web_dataset_X_test, web_dataset_y_train, web_dataset_y_test = train_test_split(web_dataset.iloc[:,:-1], web_dataset.iloc[:,-1], test_size=0.2, random_state = i)\n",
    "    web_clf = tree.DecisionTreeClassifier(max_depth=1, criterion=\"entropy\", random_state = 1221)\n",
    "    web_clf = web_clf.fit(web_dataset_X_train, web_dataset_y_train)\n",
    "    web_y_pred = web_clf.predict(web_dataset_X_test)\n",
    "    web_test_acc_ds = metrics.accuracy_score(web_dataset_y_test, web_y_pred)\n",
    "    web_ds.append(web_test_acc_ds)\n",
    "    i += 1\n",
    "for i in range(1, 20):\n",
    "    web_dataset_X_train, web_dataset_X_test, web_dataset_y_train, web_dataset_y_test = train_test_split(web_dataset.iloc[:,:-1], web_dataset.iloc[:,-1], test_size=0.2, random_state = i)\n",
    "    web_clf = tree.DecisionTreeClassifier()\n",
    "    web_clf = web_clf.fit(web_dataset_X_train, web_dataset_y_train)\n",
    "    web_y_pred = web_clf.predict(web_dataset_X_test)\n",
    "    web_test_acc_uds = metrics.accuracy_score(web_dataset_y_test, web_y_pred)\n",
    "    web_uds.append(web_test_acc_uds)\n",
    "    i += 1\n",
    "for i in range(1, 20):\n",
    "    web_dataset_X_train, web_dataset_X_test, web_dataset_y_train, web_dataset_y_test = train_test_split(web_dataset.iloc[:,:-1], web_dataset.iloc[:,-1], test_size=0.2, random_state = i)\n",
    "    web_clf = tree.DecisionTreeClassifier(max_depth=5 ,criterion=\"entropy\", random_state = 1221)\n",
    "    web_clf = web_clf.fit(web_dataset_X_train, web_dataset_y_train)\n",
    "    web_y_pred = web_clf.predict(web_dataset_X_test)\n",
    "    web_test_acc_pds = metrics.accuracy_score(web_dataset_y_test, web_y_pred)\n",
    "    web_pds.append(web_test_acc_pds)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_dsvsuds_res = stats.ttest_rel(arr_ds, arr_uds)\n",
    "arr_dsvspds_res = stats.ttest_rel(arr_ds, arr_pds)\n",
    "arr_udsvspds_res = stats.ttest_rel(arr_uds, arr_pds)\n",
    "\n",
    "bcp_dsvsuds_res = stats.ttest_rel(bcp_ds, bcp_uds)\n",
    "bcp_dsvspds_res = stats.ttest_rel(bcp_ds, bcp_pds)\n",
    "bcp_udsvspds_res = stats.ttest_rel(bcp_uds, bcp_pds)\n",
    "\n",
    "web_dsvsuds_res = stats.ttest_rel(web_ds, web_uds)\n",
    "web_dsvspds_res = stats.ttest_rel(web_ds, web_pds)\n",
    "web_udsvspds_res = stats.ttest_rel(web_uds, web_pds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---** arrhythmia dataset **---\n",
      "DisicionStump Mean:  0.4725274725274725\n",
      "UnprunedDisicionTree Mean:  0.6373626373626373\n",
      "PrunedDisicionTree Mean:  0.6593406593406593\n",
      "DT vs UnpruDT Siginificant TtestResult(statistic=-5.108108108108109, pvalue=7.351576660173747e-05, df=18)\n",
      "DT vs PruDT Siginificant TtestResult(statistic=-10.7234440001098, pvalue=3.0166701466465686e-09, df=18)\n",
      "UnpruDT vs PruDT Siginificant TtestResult(statistic=-1.8654375603358617, pvalue=0.07850534048428909, df=18)\n",
      "---** BCP dataset **---\n",
      "DisicionStump Mean:  0.927007299270073\n",
      "UnprunedDisicionTree Mean:  0.9416058394160584\n",
      "PrunedDisicionTree Mean:  0.9635036496350365\n",
      "DT vs UnpruDT Siginificant TtestResult(statistic=-5.37204762688507, pvalue=4.179752430846729e-05, df=18)\n",
      "DT vs PruDT Siginificant TtestResult(statistic=-3.9274583611376803, pvalue=0.000987131829633203, df=18)\n",
      "UnpruDT vs PruDT Siginificant TtestResult(statistic=0.09687505255809678, pvalue=0.9238961215097071, df=18)\n",
      "---** website-phishing dataset **---\n",
      "DisicionStump Mean:  0.8959746720940751\n",
      "UnprunedDisicionTree Mean:  0.9624604251469923\n",
      "PrunedDisicionTree Mean:  0.9176843057440073\n",
      "DT vs UnpruDT Siginificant TtestResult(statistic=-45.095000452906824, pvalue=5.737107016370956e-20, df=18)\n",
      "DT vs PruDT Siginificant TtestResult(statistic=-38.14113403366115, pvalue=1.1349550422429962e-18, df=18)\n",
      "UnpruDT vs PruDT Siginificant TtestResult(statistic=27.77569207105257, pvalue=3.1197179157730267e-16, df=18)\n"
     ]
    }
   ],
   "source": [
    "print(\"---** arrhythmia dataset **---\")\n",
    "print(\"DisicionStump Mean: \", amean_ds)\n",
    "print(\"UnprunedDisicionTree Mean: \", amean_uds)\n",
    "print(\"PrunedDisicionTree Mean: \", amean_pds)\n",
    "print(\"DT vs UnpruDT Siginificant\", arr_dsvsuds_res)\n",
    "print(\"DT vs PruDT Siginificant\", arr_dsvspds_res)\n",
    "print(\"UnpruDT vs PruDT Siginificant\", arr_udsvspds_res)\n",
    "print(\"---** BCP dataset **---\")\n",
    "print(\"DisicionStump Mean: \", bmean_ds)\n",
    "print(\"UnprunedDisicionTree Mean: \", bmean_uds)\n",
    "print(\"PrunedDisicionTree Mean: \", bmean_pds)\n",
    "print(\"DT vs UnpruDT Siginificant\", bcp_dsvsuds_res)\n",
    "print(\"DT vs PruDT Siginificant\", bcp_dsvspds_res)\n",
    "print(\"UnpruDT vs PruDT Siginificant\", bcp_udsvspds_res)\n",
    "print(\"---** website-phishing dataset **---\")\n",
    "print(\"DisicionStump Mean: \", wmean_ds)\n",
    "print(\"UnprunedDisicionTree Mean: \", wmean_uds)\n",
    "print(\"PrunedDisicionTree Mean: \", wmean_pds)\n",
    "print(\"DT vs UnpruDT Siginificant\", web_dsvsuds_res)\n",
    "print(\"DT vs PruDT Siginificant\", web_dsvspds_res)\n",
    "print(\"UnpruDT vs PruDT Siginificant\", web_udsvspds_res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"4\">In this task, we will compare the three methods by computing means and p-values for each model on each data set.\n",
    "\n",
    "In order to avoid the accuracy rate due to luck, we run the model of all data sets 20 times, record the accuracy rate of each time and calculate the mean value.\n",
    "\n",
    "In the \"arrhythmia.csv\" dataset, both the unpruned and pruned decision trees are more accurate than the decision stumps. All three models performed poorly, with means below 80%. Only unpruned and pruned decision trees are not significantly different. As we said in the last task, this may be because there are not enough samples to be used effectively.\n",
    "\n",
    "In the ''BCP.csv\" data set, the accuracy of the unpruned decision tree and the pruned decision tree is higher than that of the decision stump. The decision stump is significantly different from the unpruned decision tree and the pruned decision tree. The performance of the three models is very good, the average accuracy rate is above 90%.\n",
    "\n",
    "In the \"website-phishing.csv\" dataset, all three models differ significantly. All three models also performed well.\n",
    "\n",
    "Based on our observations of the performance of each model on each dataset, decision stump performed the worst, probably because it is less sophisticated than the other two in terms of capturing patterns in the data.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">Task 5:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrhythmia dataset\n",
      "----*****-----\n",
      "Number of nodes in the last tree is: 1 with ccp_alpha: 0.0680611748317167\n",
      "arrhythmia dataset test set accuracy : 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "#arrhythmia\n",
    "print(\"arrhythmia dataset\")\n",
    "print(\"----*****-----\")\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "arr_path = clf.cost_complexity_pruning_path(arr_dataset_X_train, arr_dataset_y_train)\n",
    "ccp_alphas, impurities = arr_path.ccp_alphas, arr_path.impurities\n",
    "\n",
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(arr_dataset_X_train, arr_dataset_y_train)\n",
    "    clfs.append(clf)\n",
    "print(\n",
    "    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "        clfs[-1].tree_.node_count, ccp_alphas[-1]\n",
    "    )\n",
    ")\n",
    "\n",
    "arr_clf = DecisionTreeClassifier(ccp_alpha = ccp_alphas[-1])\n",
    "arr_clf = arr_clf.fit(arr_dataset_X_train, arr_dataset_y_train)\n",
    "arr_y_pred = arr_clf.predict(arr_dataset_X_test)\n",
    "acc_accu = metrics.accuracy_score(arr_dataset_y_test, arr_y_pred)\n",
    "print(\"arrhythmia dataset test set accuracy :\", acc_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCP dataset\n",
      "----*****-----\n",
      "Number of nodes in the last tree is: 1 with ccp_alpha: 0.3299884482094724\n",
      "BCP dataset test set accuracy : 0.6788321167883211\n"
     ]
    }
   ],
   "source": [
    "#BCP\n",
    "print(\"BCP dataset\")\n",
    "print(\"----*****-----\")\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "bcp_path = clf.cost_complexity_pruning_path(bcp_dataset_X_train, bcp_dataset_y_train)\n",
    "ccp_alphas, impurities = bcp_path.ccp_alphas, bcp_path.impurities\n",
    "\n",
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(bcp_dataset_X_train, bcp_dataset_y_train)\n",
    "    clfs.append(clf)\n",
    "print(\n",
    "    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "        clfs[-1].tree_.node_count, ccp_alphas[-1]\n",
    "    )\n",
    ")\n",
    "\n",
    "bcp_clf = DecisionTreeClassifier(ccp_alpha = ccp_alphas[-1])\n",
    "bcp_clf = bcp_clf.fit(bcp_dataset_X_train, bcp_dataset_y_train)\n",
    "bcp_y_pred = bcp_clf.predict(bcp_dataset_X_test)\n",
    "bcp_accu = metrics.accuracy_score(bcp_dataset_y_test, bcp_y_pred)\n",
    "print(\"BCP dataset test set accuracy :\", bcp_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "website-phishin dataset\n",
      "----*****-----\n",
      "Number of nodes in the last tree is: 1 with ccp_alpha: 0.2982770794526449\n",
      "BCP dataset test set accuracy : 0.5707824513794663\n"
     ]
    }
   ],
   "source": [
    "#web\n",
    "print(\"website-phishin dataset\")\n",
    "print(\"----*****-----\")\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "web_path = clf.cost_complexity_pruning_path(web_dataset_X_train, web_dataset_y_train)\n",
    "ccp_alphas, impurities = web_path.ccp_alphas, web_path.impurities\n",
    "\n",
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(web_dataset_X_train, web_dataset_y_train)\n",
    "    clfs.append(clf)\n",
    "print(\n",
    "    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "        clfs[-1].tree_.node_count, ccp_alphas[-1]\n",
    "    )\n",
    ")\n",
    "\n",
    "web_clf = DecisionTreeClassifier(ccp_alpha = ccp_alphas[-1])\n",
    "web_clf = bcp_clf.fit(web_dataset_X_train, web_dataset_y_train)\n",
    "web_y_pred = web_clf.predict(web_dataset_X_test)\n",
    "web_accu = metrics.accuracy_score(web_dataset_y_test, web_y_pred)\n",
    "print(\"BCP dataset test set accuracy :\", web_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the mean\n",
    "amean_pods = acc_accu.mean() #arrhythmia\n",
    "bmean_pods = bcp_accu.mean() #BCP\n",
    "wmean_pods = web_accu.mean() #web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_pods = []\n",
    "bcp_pods = []\n",
    "web_pods = []\n",
    "i = 1\n",
    "\n",
    "for i in range(1, 20):\n",
    "    arr_dataset_X_train, arr_dataset_X_test, arr_dataset_y_train, arr_dataset_y_test = train_test_split(arr_dataset.iloc[:,:-1], arr_dataset.iloc[:,-1], test_size=0.2, random_state = i)\n",
    "    arr_clf = DecisionTreeClassifier(ccp_alpha = 0.0680611748317167)\n",
    "    arr_clf = arr_clf.fit(arr_dataset_X_train, arr_dataset_y_train)\n",
    "    arr_y_pred = arr_clf.predict(arr_dataset_X_test)\n",
    "    acc_accu = metrics.accuracy_score(arr_dataset_y_test, arr_y_pred)\n",
    "    arr_pods.append(acc_accu)\n",
    "    i += 1\n",
    "for i in range(1, 20):\n",
    "    bcp_dataset_X_train, bcp_dataset_X_test, bcp_dataset_y_train, bcp_dataset_y_test = train_test_split(bcp_dataset.iloc[:,:-1], bcp_dataset.iloc[:,-1], test_size=0.2, random_state = i)\n",
    "    bcp_clf = DecisionTreeClassifier(ccp_alpha = 0.3299884482094724)\n",
    "    bcp_clf = bcp_clf.fit(bcp_dataset_X_train, bcp_dataset_y_train)\n",
    "    bcp_y_pred = bcp_clf.predict(bcp_dataset_X_test)\n",
    "    bcp_accu = metrics.accuracy_score(bcp_dataset_y_test, bcp_y_pred)\n",
    "    bcp_pods.append(bcp_accu)\n",
    "    i += 1\n",
    "for i in range(1, 20):\n",
    "    web_dataset_X_train, web_dataset_X_test, web_dataset_y_train, web_dataset_y_test = train_test_split(web_dataset.iloc[:,:-1], web_dataset.iloc[:,-1], test_size=0.2, random_state = i)\n",
    "    web_clf = DecisionTreeClassifier(ccp_alpha = 0.2982770794526449)\n",
    "    web_clf = bcp_clf.fit(web_dataset_X_train, web_dataset_y_train)\n",
    "    web_y_pred = web_clf.predict(web_dataset_X_test)\n",
    "    web_accu = metrics.accuracy_score(web_dataset_y_test, web_y_pred)\n",
    "    web_pods.append(web_accu)\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_pdsvspods_res = stats.ttest_rel(arr_pds, arr_pods)\n",
    "bcp_pdsvspods_res = stats.ttest_rel(bcp_pds, bcp_pods)\n",
    "web_pdsvspods_res = stats.ttest_rel(web_pds, web_pods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---** arrhythmia dataset **---\n",
      "PrePrunedDisicionTree Mean:  0.6593406593406593\n",
      "PostPrunedDisicionTree Mean:  0.5714285714285714\n",
      "PrePruDT vs PostPruDT Siginificant TtestResult(statistic=10.93766468831085, pvalue=2.2103456067991915e-09, df=18)\n",
      "---** BCP dataset **---\n",
      "PrePrunedDisicionTree Mean:  0.9635036496350365\n",
      "PostPrunedDisicionTree Mean:  0.6788321167883211\n",
      "PrePruDT vs PostPruDT Siginificant TtestResult(statistic=13.374912431005576, pvalue=8.637872006028704e-11, df=18)\n",
      "---** website-phishing dataset **---\n",
      "PrePrunedDisicionTree Mean:  0.9176843057440073\n",
      "PostPrunedDisicionTree Mean:  0.5707824513794663\n",
      "PrePruDT vs PostPruDT Siginificant TtestResult(statistic=115.57598586748612, pvalue=2.686026865532714e-27, df=18)\n"
     ]
    }
   ],
   "source": [
    "print(\"---** arrhythmia dataset **---\")\n",
    "print(\"PrePrunedDisicionTree Mean: \", amean_pds)\n",
    "print(\"PostPrunedDisicionTree Mean: \", amean_pods)\n",
    "print(\"PrePruDT vs PostPruDT Siginificant\", arr_pdsvspods_res)\n",
    "print(\"---** BCP dataset **---\")\n",
    "print(\"PrePrunedDisicionTree Mean: \", bmean_pds)\n",
    "print(\"PostPrunedDisicionTree Mean: \", bmean_pods)\n",
    "print(\"PrePruDT vs PostPruDT Siginificant\", bcp_pdsvspods_res)\n",
    "print(\"---** website-phishing dataset **---\")\n",
    "print(\"PrePrunedDisicionTree Mean: \", wmean_pds)\n",
    "print(\"PostPrunedDisicionTree Mean: \", wmean_pods)\n",
    "print(\"PrePruDT vs PostPruDT Siginificant\", web_pdsvspods_res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"4\">We used the pre-pruning technique in task 2, so in this task we will use post-pruning. Post-pruning is the pruning of a tree after it has been classified. From the explanation on the scikit-learn official website, we can know that \"ccp_alpha\" is \"minimum cost complexity pruning recursively finds the node with the ‘weakest link’ \". In order to know which ''ccp_alpha'' values are suitable, we will use .cost_complexity_pruning_path(). The most suitable ''ccp_alpha'' value is the last alpha value to prune the whole tree. After finding the most suitable value, we will use and The same process as in task 4 is used to compare the impact of these two pruning strategies on the three data sets.\n",
    "\n",
    "We can see that there are significant differences between the two methods across the three datasets. And obviously, the accuracy of pre-pruning is much higher than that of post-pruning. In this case, it may be because the number of calculated ccp_alpha values is insufficient for the three datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ee113b9407e8e84651039a293784ccbf67984651a402226c26c8e0e37d6e692"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
