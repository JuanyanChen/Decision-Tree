{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"12\">Compsci 361 Assignment 1</font>\n",
    "\n",
    "<font size = \"5\"> Name: Joanne Chen </font>\n",
    "\n",
    "<font size = \"5\">UPI: jehc820 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 73, 'name': 'Mushroom', 'repository_url': 'https://archive.ics.uci.edu/dataset/73/mushroom', 'data_url': 'https://archive.ics.uci.edu/static/public/73/data.csv', 'abstract': 'From Audobon Society Field Guide; mushrooms described in terms of physical characteristics; classification: poisonous or edible', 'area': 'Biology', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 8124, 'num_features': 22, 'feature_types': ['Categorical'], 'demographics': [], 'target_col': ['poisonous'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1981, 'last_updated': 'Thu Aug 10 2023', 'dataset_doi': '10.24432/C5959T', 'creators': [], 'intro_paper': None, 'additional_info': {'summary': \"This data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family (pp. 500-525).  Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended.  This latter class was combined with the poisonous one.  The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like ``leaflets three, let it be'' for Poisonous Oak and Ivy.\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '     1. cap-shape:                bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\\r\\n     2. cap-surface:              fibrous=f,grooves=g,scaly=y,smooth=s\\r\\n     3. cap-color:                brown=n,buff=b,cinnamon=c,gray=g,green=r, pink=p,purple=u,red=e,white=w,yellow=y\\r\\n     4. bruises?:                 bruises=t,no=f\\r\\n     5. odor:                     almond=a,anise=l,creosote=c,fishy=y,foul=f, musty=m,none=n,pungent=p,spicy=s\\r\\n     6. gill-attachment:          attached=a,descending=d,free=f,notched=n\\r\\n     7. gill-spacing:             close=c,crowded=w,distant=d\\r\\n     8. gill-size:                broad=b,narrow=n\\r\\n     9. gill-color:               black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e, white=w,yellow=y\\r\\n    10. stalk-shape:              enlarging=e,tapering=t\\r\\n    11. stalk-root:               bulbous=b,club=c,cup=u,equal=e, rhizomorphs=z,rooted=r,missing=?\\r\\n    12. stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\\r\\n    13. stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\\r\\n    14. stalk-color-above-ring:   brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y\\r\\n    15. stalk-color-below-ring:   brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y\\r\\n    16. veil-type:                partial=p,universal=u\\r\\n    17. veil-color:               brown=n,orange=o,white=w,yellow=y\\r\\n    18. ring-number:              none=n,one=o,two=t\\r\\n    19. ring-type:                cobwebby=c,evanescent=e,flaring=f,large=l, none=n,pendant=p,sheathing=s,zone=z\\r\\n    20. spore-print-color:        black=k,brown=n,buff=b,chocolate=h,green=r, orange=o,purple=u,white=w,yellow=y\\r\\n    21. population:               abundant=a,clustered=c,numerous=n, scattered=s,several=v,solitary=y\\r\\n    22. habitat:                  grasses=g,leaves=l,meadows=m,paths=p, urban=u,waste=w,woods=d', 'citation': None}}\n",
      "                        name     role         type demographic  \\\n",
      "0                  poisonous   Target  Categorical        None   \n",
      "1                  cap-shape  Feature  Categorical        None   \n",
      "2                cap-surface  Feature  Categorical        None   \n",
      "3                  cap-color  Feature       Binary        None   \n",
      "4                    bruises  Feature  Categorical        None   \n",
      "5                       odor  Feature  Categorical        None   \n",
      "6            gill-attachment  Feature  Categorical        None   \n",
      "7               gill-spacing  Feature  Categorical        None   \n",
      "8                  gill-size  Feature  Categorical        None   \n",
      "9                 gill-color  Feature  Categorical        None   \n",
      "10               stalk-shape  Feature  Categorical        None   \n",
      "11                stalk-root  Feature  Categorical        None   \n",
      "12  stalk-surface-above-ring  Feature  Categorical        None   \n",
      "13  stalk-surface-below-ring  Feature  Categorical        None   \n",
      "14    stalk-color-above-ring  Feature  Categorical        None   \n",
      "15    stalk-color-below-ring  Feature  Categorical        None   \n",
      "16                 veil-type  Feature       Binary        None   \n",
      "17                veil-color  Feature  Categorical        None   \n",
      "18               ring-number  Feature  Categorical        None   \n",
      "19                 ring-type  Feature  Categorical        None   \n",
      "20         spore-print-color  Feature  Categorical        None   \n",
      "21                population  Feature  Categorical        None   \n",
      "22                   habitat  Feature  Categorical        None   \n",
      "\n",
      "                                          description units missing_values  \n",
      "0                                                None  None             no  \n",
      "1   bell=b,conical=c,convex=x,flat=f, knobbed=k,su...  None             no  \n",
      "2                fibrous=f,grooves=g,scaly=y,smooth=s  None             no  \n",
      "3   brown=n,buff=b,cinnamon=c,gray=g,green=r, pink...  None             no  \n",
      "4                                      bruises=t,no=f  None             no  \n",
      "5   almond=a,anise=l,creosote=c,fishy=y,foul=f, mu...  None             no  \n",
      "6            attached=a,descending=d,free=f,notched=n  None             no  \n",
      "7                         close=c,crowded=w,distant=d  None             no  \n",
      "8                                    broad=b,narrow=n  None             no  \n",
      "9   black=k,brown=n,buff=b,chocolate=h,gray=g, gre...  None             no  \n",
      "10                             enlarging=e,tapering=t  None             no  \n",
      "11  bulbous=b,club=c,cup=u,equal=e, rhizomorphs=z,...  None            yes  \n",
      "12                 fibrous=f,scaly=y,silky=k,smooth=s  None             no  \n",
      "13                 fibrous=f,scaly=y,silky=k,smooth=s  None             no  \n",
      "14  brown=n,buff=b,cinnamon=c,gray=g,orange=o, pin...  None             no  \n",
      "15  brown=n,buff=b,cinnamon=c,gray=g,orange=o, pin...  None             no  \n",
      "16                              partial=p,universal=u  None             no  \n",
      "17                  brown=n,orange=o,white=w,yellow=y  None             no  \n",
      "18                                 none=n,one=o,two=t  None             no  \n",
      "19  cobwebby=c,evanescent=e,flaring=f,large=l, non...  None             no  \n",
      "20  black=k,brown=n,buff=b,chocolate=h,green=r, or...  None             no  \n",
      "21  abundant=a,clustered=c,numerous=n, scattered=s...  None             no  \n",
      "22  grasses=g,leaves=l,meadows=m,paths=p, urban=u,...  None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "mushroom = fetch_ucirepo(id=73) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = mushroom.data.features \n",
    "y = mushroom.data.targets \n",
    "#print(y)\n",
    "\n",
    "# metadata \n",
    "print(mushroom.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(mushroom.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "#from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">TASK A & B </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly replace all missing data with the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mushroom missing values:  2480\n"
     ]
    }
   ],
   "source": [
    "# #calculate amount of missing value in the dataset\n",
    "missing_value = X.isna().sum()\n",
    "missing_value_count = missing_value.sum()\n",
    "print(\"mushroom missing values: \", missing_value_count)\n",
    "#there are 2480 missing value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mushroom missing values:  0\n",
      "stalk-root\n",
      "b    6256\n",
      "e    1120\n",
      "c     556\n",
      "r     192\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kh/qlpm2bl96z120l3yf51tl2_m0000gn/T/ipykernel_2822/805927011.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[fea] = X[fea].fillna(mode)\n"
     ]
    }
   ],
   "source": [
    "#replace the missing value with mode\n",
    "for fea in X:\n",
    "    if X[fea].isna().sum() != 0:\n",
    "        mode = X[fea].value_counts().idxmax()\n",
    "        X[fea] = X[fea].fillna(mode)\n",
    "\n",
    "#test\n",
    "print(\"mushroom missing values: \", X.isna().sum().sum())\n",
    "#print(X[\"stalk-root\"])\n",
    "print(X[\"stalk-root\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '3'>To begin, we import the mushroom dataset. Then we read all the dataset, investigare missing data, and find out the majority class to replace all the missing values. We only found missing values in \"stalk-root\" column with 2480 missing data using .isna().sum(). We use for loop to find the mode in the column. In our case, 'b' is the mode. Then we replace all '?' with 'b'. Finally, we test the missing value again using .value_counts(). </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1955123731414904\n"
     ]
    }
   ],
   "source": [
    "#Calculates the entropy of the given data set\n",
    "#lables is a list of lables\n",
    "def cal_entropy(labels):\n",
    "    #total amount of lables\n",
    "    total_label = len(labels)\n",
    "\n",
    "    #if there is only one lable then entropy is 1\n",
    "    if total_label <= 1:\n",
    "        return 0\n",
    "    \n",
    "    #counting the amount of each lable\n",
    "    if len(np.unique(labels)) == 1:\n",
    "        return 0\n",
    "    counts = labels.value_counts()\n",
    "    count_of_e = counts['e']\n",
    "    count_of_p = counts['p']\n",
    "\n",
    "    #calculate probs of e and p\n",
    "    probs_e = count_of_e / total_label\n",
    "    probs_p = count_of_p / total_label\n",
    "\n",
    "    #return entropy\n",
    "    return -1 * (probs_e * math.log(probs_e,2) + probs_p * math.log(probs_p,2))\n",
    "\n",
    "#test with y\n",
    "#print(cal_entropy(y))\n",
    "\n",
    "#Calculates the information gain after splitting the data on the chosen attribute. \n",
    "#attr (attributes) is a part of X and labels is a part of y\n",
    "def ig(attr, labels, col_index):\n",
    "    #calculate entropy before split\n",
    "    entropy_b4_split = cal_entropy(labels)\n",
    "\n",
    "    #calculate the sum of entropy after split\n",
    "    total_attr = len(labels)\n",
    "\n",
    "    #find the unique value and its amount in the each column\n",
    "    uniq_fea = np.unique(attr.iloc[:, col_index])\n",
    "    attr_counts = attr.iloc[:, col_index].value_counts()\n",
    "    \n",
    "    #calculate sum of entropy after split\n",
    "    entropy_after_split = 0\n",
    "    for element in uniq_fea:\n",
    "        label_after_split = labels[attr.iloc[:, col_index] == element] #array of labels after split\n",
    "        entropy_after_split += attr_counts[element] / total_attr * cal_entropy(label_after_split)\n",
    "\n",
    "    return entropy_b4_split - entropy_after_split\n",
    "\n",
    "#test with X and y\n",
    "print(ig(ms_dataset_X_train,ms_dataset_y_train,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '3'> In this part, we use NumPY and Pandas to define entropy and information gain method. These two method will be used in the DecisionTreeClassifier.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'odor': {'a': {'cap-shape': {'b': 'e', 'f': 'e', 'x': 'e'}}, 'c': 'p', 'f': {'cap-shape': {'f': 'p', 'k': 'p', 'x': 'p'}}, 'l': {'cap-shape': {'b': 'e', 'f': 'e', 'x': 'e'}}, 'm': {'cap-shape': {'f': 'p', 'k': 'p', 'x': 'p'}}, 'n': {'spore-print-color': {'b': 'e', 'h': 'e', 'k': 'e', 'n': 'e', 'o': 'e', 'r': 'p', 'w': 'e', 'y': 'e'}}, 'p': {'cap-shape': {'f': 'p', 'x': 'p'}}, 's': {'cap-shape': {'f': 'p', 'k': 'p', 'x': 'p'}}, 'y': {'cap-shape': {'f': 'p', 'k': 'p', 'x': 'p'}}}}\n"
     ]
    }
   ],
   "source": [
    "#we calculate the info gain of all attributes (all columns) recursively. \n",
    "#We will pick the attribute with highest score for the first split. \n",
    "\n",
    "def build_decision_tree(X, y, current_depth, stopping_depth):\n",
    "    max_ig = -1\n",
    "    best_fea_idx = 0 #record the index of attribute with the highest score of ig\n",
    "\n",
    "    #get best feature with highest ig\n",
    "    num_row, num_col = X.shape\n",
    "    for col_idx in range(num_col):\n",
    "        fea_ig = ig(X, y, col_idx)\n",
    "        if fea_ig > max_ig:\n",
    "            max_ig = fea_ig\n",
    "            best_fea_idx = col_idx\n",
    "\n",
    "    #find the best attribute column\n",
    "    best_fea_df = X.iloc[:,best_fea_idx]\n",
    "    best_feature = X.columns[best_fea_idx]\n",
    "\n",
    "    #seperate all row number(index) by unique attributes\n",
    "    uniq_fea_list = np.unique(best_fea_df)\n",
    "\n",
    "    #if only one column left or\n",
    "    #if we go thru all features or \n",
    "    #if reach to the target depth (depth control) then stop recursion and return the predicted result\n",
    "    if (len(uniq_fea_list) <= 1) or (num_col == 0) or (current_depth > stopping_depth):\n",
    "        labels_array, label_count = np.unique(y, return_counts=True) #labels_array is an array of 'e' and 'p'\n",
    "        return labels_array[np.argmax(label_count)]  #return 'e' or 'p' by investigating which one appear more frequently\n",
    "\n",
    "    #start to build the decision tree; make a dictionary to store the root\n",
    "    result_tree_dict = {best_feature:{}}\n",
    "\n",
    "    for fea in uniq_fea_list:\n",
    "\n",
    "        #find all row number of a type of unique feature\n",
    "        satisfied_index = X.index[best_fea_df == fea].tolist() \n",
    "\n",
    "        #drop the used column? #X_sub = X.drop(columns=[best_feature]).loc[satisfied_index] #eg all 'a' stumps in the best col\n",
    "        X_sub = X.loc[satisfied_index]\n",
    "        y_sub = y.loc[satisfied_index]\n",
    "        #list and tuple method #result_tree_dict[best_feature].append((fea, build_decision_tree(X_sub, y_sub, current_depth+1, stopping_depth)))\n",
    "        result_tree_dict[best_feature][fea] = build_decision_tree(X_sub, y_sub, current_depth+1, stopping_depth)\n",
    "        #use dict to store n-ary tree stumps\n",
    "\n",
    "    return result_tree_dict\n",
    "\n",
    "\n",
    "#TEST\n",
    "#print(split_data(X,y))\n",
    "print(build_decision_tree(X,y,0,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a decision tree using information gain to pick the best feature. Then we seperate the dataframe into different subset based on the different attribute, like 'a', 'f'. Then we recursively calculate the best feature with the highest information gain.\n",
    "\n",
    "We also have depth control in this build_decision_tree method. Stopping_depth is a parameter of this method, so we can input whatever depth or level we want to stop the recursion (calculation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">TASK C </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into train and test data\n",
    "ms_dataset_X_train, ms_dataset_X_test, ms_dataset_y_train, ms_dataset_y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "#print(ms_dataset_X_train)\n",
    "#print(ms_dataset_X_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat']\n"
     ]
    }
   ],
   "source": [
    "feature_name_list = list(mushroom.variables.name)[1:]\n",
    "print(feature_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'odor': {'a': {'cap-shape': {'b': 'e', 'f': 'e', 'x': 'e'}}, 'c': 'p', 'f': {'cap-shape': {'f': 'p', 'k': 'p', 'x': 'p'}}, 'l': {'cap-shape': {'b': 'e', 'f': 'e', 'x': 'e'}}, 'm': {'cap-shape': {'f': 'p', 'k': 'p', 'x': 'p'}}, 'n': {'spore-print-color': {'b': 'e', 'h': 'e', 'k': 'e', 'n': 'e', 'o': 'e', 'r': 'p', 'w': 'e', 'y': 'e'}}, 'p': {'cap-shape': {'f': 'p', 'x': 'p'}}, 's': {'cap-shape': {'f': 'p', 'k': 'p', 'x': 'p'}}, 'y': {'cap-shape': {'f': 'p', 'k': 'p', 'x': 'p'}}}}\n"
     ]
    }
   ],
   "source": [
    "trained_tree = build_decision_tree(ms_dataset_X_train,ms_dataset_y_train,0,1)\n",
    "print(trained_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '3'> In this part, we split data into training data and test data. 80% of the total data is training data and 20% is the test dataset. I choose this method to split the data because the data is big enough and 80%/20% method that only run the test once is the fastest and easiest way to test the data.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p']\n"
     ]
    }
   ],
   "source": [
    "def get_instance_predict(tree, test_instance): #test_instance is a pd Series object\n",
    "    if isinstance(tree, dict):  #if subtree is dict, we continue, otherwise it is label\n",
    "        feature_name = list(tree.keys())[0] #feature, 'odor'\n",
    "        subtree = tree[feature_name] #attribute, a/f/l...\n",
    "        instance = test_instance[feature_name] #find the attribute of test_instance using column name\n",
    "        if instance in subtree:\n",
    "            return get_instance_predict(subtree[instance], test_instance)\n",
    "        else:\n",
    "            return 'UNKNOWN' #return UNKNOW if there is no matched instance in the trained tree\n",
    "    return tree\n",
    "\n",
    "def get_df_predict_result(trained_tree, X_test):\n",
    "    pred_result = []\n",
    "    #get all feature in every row in the test data\n",
    "    for index, instance in X_test.iterrows(): #iterate over each rows of the test dataframe and get the instance\n",
    "        pred_result.append(get_instance_predict(tree, instance))\n",
    "    return pred_result\n",
    "\n",
    "print(get_df_predict_result(trained_tree, ms_dataset_X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a test procedure using the training data we split before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">TASK D </font>\n",
    "\n",
    "Task D: We pick accuracy as the evaluation method for my build_decision_tree algorithm. Accuracy = (TP + TN) / (P + N)\n",
    "\n",
    "We choose this matrix because database is big enough and this evaluation method is easy to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p']\n"
     ]
    }
   ],
   "source": [
    "y_true= ms_dataset_y_test[\"poisonous\"].tolist()\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'e', 'p', 'p', 'e', 'p', 'e', 'e', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'e', 'p', 'p', 'p', 'e', 'e', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'p', 'p', 'p']\n"
     ]
    }
   ],
   "source": [
    "y_test = get_df_predict_result(trained_tree, ms_dataset_X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.32307692307693\n"
     ]
    }
   ],
   "source": [
    "#Calculates the accuracy of the model predictions\n",
    "#y_true is the list of true labels; y_test is the list of predictions\n",
    "def calculate_accuracy(y_true, y_test):\n",
    "    correct_counts = np.sum(np.array(y_test) == np.array(y_true))\n",
    "    total = len(y_true)\n",
    "    return correct_counts / total * 100\n",
    "\n",
    "acc = calculate_accuracy(y_true,y_test)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
